# translation_engine.py
# Core translation logic extracted from PPT_Language_Change.py

import os
import re
import time
from pptx import Presentation
from pptx.dml.color import RGBColor
import openai


# ---------- [ì„œì‹ ë³´ì¡´ì„ ìœ„í•œ íƒœê¹…/ë³µì› ìœ í‹¸] ----------
RUN_TAG = re.compile(r"\[\[R(\d+)\]\]|\[\[/R(\d+)\]\]")

# ==== [ì„¤ì •] =====================================================
SORRY_PATTERNS = [
    "i'm sorry", "i am sorry", "sorry, but", "apologize",
    "ì£„ì†¡í•˜ì§€ë§Œ", "ì£„ì†¡í•©ë‹ˆë‹¤", "ë²ˆì—­í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"
]

LANG_OPTIONS = [
    "English",
    "Indonesian",
    "Italian",
    "French",
    "Spanish",
    "Korean",
    "Japanese",
    "Russian",
    "German",
    "Portuguese",
    "Chinese (Simplified)",
    "Chinese (Traditional)",
]

TONE_OPTIONS = [
    "ê¸°ë³¸ê°’",
    "Med/Pharma Pro (20y)",   # ì˜ë£Œê¸°ê¸°/ì „ë¬¸ì•½ì‚¬ 20ë…„ ì „ë¬¸ê°€
    "Medical/Science Expert", # ì˜í•™/ê³¼í•™ ë²ˆì—­ ì „ë¬¸ê°€
    "Beauty Pro (20y, chic)", # ì„¸ë ¨ëœ ë·°í‹° 20ë…„ ì „ë¬¸ê°€
    "GenZ Female (20s)",      # 20ëŒ€ ì—¬ì„± íƒ€ê¹ƒ
]

OPENAI_MODEL = "gpt-5"
DEEPSEEK_MODEL = "deepseek-chat"
SLEEP_SEC = 0


def is_effectively_empty_tagged(tagged_text: str) -> bool:
    """[[R#]]ë§ˆì»¤ë¥¼ ì œê±°í•˜ê³  ë‚¨ëŠ” ì½˜í…ì¸ ê°€ ì‹¤ì§ˆì ìœ¼ë¡œ ë¹„ì—ˆëŠ”ì§€ íŒë‹¨"""
    stripped = RUN_TAG.sub("", tagged_text)  # ë§ˆì»¤ ì œê±°
    return stripped.strip() == ""  # ê³µë°±ë§Œ ë‚¨ìœ¼ë©´ ë¹ˆ ê²ƒìœ¼ë¡œ ê°„ì£¼


def looks_like_apology(text: str) -> bool:
    low = (text or "").lower()
    return any(p in low for p in SORRY_PATTERNS)


def unique_path(path: str) -> str:  
    if not os.path.exists(path):
        return path
    base, ext = os.path.splitext(path)
    i = 1
    while True:
        candidate = f"{base} ({i}){ext}"
        if not os.path.exists(candidate):
            return candidate
        i += 1


def create_openai_client(api_key: str):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return openai.OpenAI(api_key=api_key)


def create_deepseek_client(api_key: str):
    """DeepSeek í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return openai.OpenAI(
        api_key=api_key,
        base_url="https://api.deepseek.com"
    )


def safe_request(client, prompt, retries=3, delay=3, use_deepseek=False):
    for attempt in range(retries):
        try:
            model = DEEPSEEK_MODEL if use_deepseek else OPENAI_MODEL
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                timeout=60,
            )
            content = ""
            if resp and hasattr(resp, "choices") and resp.choices:
                content = getattr(resp.choices[0].message, "content", "") or ""
            if content:
                return content.strip()
        except Exception as e:
            print(f"âš ï¸ Error (attempt {attempt+1}): {e}")
            with open("error.log", "a", encoding="utf-8") as f:
                f.write(f"[Attempt {attempt+1}] {e}\n")
            time.sleep(delay)
    return ""


def _font_attrs(run):
    f = run.font
    # ê°’ ê·¸ëŒ€ë¡œ ë³´ì¡´(None í¬í•¨)
    name = f.name                 # Noneì´ë©´ í…Œë§ˆ/ë§ˆìŠ¤í„° ìƒì†
    size = f.size.pt if f.size else None
    bold = f.bold                 # True/False/None
    italic = f.italic             # True/False/None
    underline = f.underline       # True/False/None

    rgb = None
    if f.color is not None and getattr(f.color, "rgb", None) is not None:
        rgb = (f.color.rgb[0], f.color.rgb[1], f.color.rgb[2])

    return {"name": name, "size": size, "bold": bold, "italic": italic,
            "underline": underline, "rgb": rgb}


def _apply_font_attrs(run, attrs):
    from pptx.util import Pt
    f = run.font

    if attrs.get("name") is not None:
        f.name = attrs["name"]
    if attrs.get("size") is not None:
        f.size = Pt(attrs["size"])
    if attrs.get("bold") is not None:
        f.bold = attrs["bold"]
    if attrs.get("italic") is not None:
        f.italic = attrs["italic"]
    if attrs.get("underline") is not None:
        f.underline = attrs["underline"]
    if attrs.get("rgb") is not None:
        r, g, b = attrs["rgb"]
        f.color.rgb = RGBColor(r, g, b)


def adjust_font_size_for_translation(original_size, font_scale):
    """ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì˜ í°íŠ¸ í¬ê¸°ë¥¼ ì¡°ì •"""
    if original_size is None:
        return None
    return original_size * font_scale


def _apply_font_attrs_with_scale(run, attrs, font_scale=1.0):
    """í°íŠ¸ ì†ì„±ì„ ì ìš©í•˜ë˜ í¬ê¸°ë¥¼ ì¡°ì •"""
    from pptx.util import Pt
    f = run.font

    if attrs.get("name") is not None:
        f.name = attrs["name"]
    
    # í°íŠ¸ í¬ê¸° ì¡°ì • ì ìš©
    if attrs.get("size") is not None:
        adjusted_size = adjust_font_size_for_translation(attrs["size"], font_scale)
        f.size = Pt(adjusted_size)
    
    if attrs.get("bold") is not None:
        f.bold = attrs["bold"]
    if attrs.get("italic") is not None:
        f.italic = attrs["italic"]
    if attrs.get("underline") is not None:
        f.underline = attrs["underline"]
    if attrs.get("rgb") is not None:
        r, g, b = attrs["rgb"]
        f.color.rgb = RGBColor(r, g, b)


def tag_paragraph(paragraph):
    text_parts, style_map, idx = [], {}, 1
    for run in paragraph.runs:
        t = run.text or ""
        if not t:
            continue
        style_map[idx] = _font_attrs(run)
        text_parts.append(f"[[R{idx}]]{t}[[/R{idx}]]")
        idx += 1
    return "".join(text_parts), style_map


def rebuild_paragraph_from_tagged(paragraph, translated, style_map, font_scale=1.0):
    while paragraph.runs:
        paragraph.runs[0]._r.getparent().remove(paragraph.runs[0]._r)

    tokens, pos = [], 0
    for m in RUN_TAG.finditer(translated):
        s, e = m.span()
        if s > pos:
            tokens.append(("text", translated[pos:s]))
        if m.group(1):
            tokens.append(("start", int(m.group(1))))
        if m.group(2):
            tokens.append(("end", int(m.group(2))))
        pos = e
    if pos < len(translated):
        tokens.append(("text", translated[pos:]))

    out_runs, stack, buffer = [], [], {}
    for ttype, value in tokens:
        if ttype == "start":
            stack.append(value)
            buffer.setdefault(value, [])
        elif ttype == "end":
            if stack and stack[-1] == value:
                stack.pop()
            joined = "".join(buffer.get(value, []))
            out_runs.append((value, joined))
            buffer[value] = []
        else:
            if stack:
                buffer[stack[-1]].append(value)
            else:
                out_runs.append((None, value))

    for run_id, buf in buffer.items():
        if buf:
            out_runs.append((run_id, "".join(buf)))

    if not out_runs:
        r = paragraph.add_run()
        r.text = translated
        return

    for run_id, txt in out_runs:
        if not txt:
            continue
        r = paragraph.add_run()
        r.text = txt
        if run_id and run_id in style_map:
            if font_scale != 1.0:
                _apply_font_attrs_with_scale(r, style_map[run_id], font_scale)
            else:
                _apply_font_attrs(r, style_map[run_id])


def _parse_run_chunks(translated):
    # R# â†’ í…ìŠ¤íŠ¸ ë§¤í•‘ê³¼, ë§ˆì»¤ ë°– í…ìŠ¤íŠ¸ ì¡´ì¬ ì—¬ë¶€ë¥¼ íŒì •
    ids = []
    chunks = {}
    stack = []
    buf = {}
    outside = []

    pos = 0
    for m in RUN_TAG.finditer(translated):
        s, e = m.span()
        if s > pos:
            if stack:
                buf.setdefault(stack[-1], []).append(translated[pos:s])
            else:
                outside.append(translated[pos:s])
        if m.group(1):  # [[R#]]
            rid = int(m.group(1)); stack.append(rid); ids.append(rid)
        if m.group(2):  # [[/R#]]
            rid = int(m.group(2))
            if stack and stack[-1] == rid:
                stack.pop()
                joined = "".join(buf.get(rid, []))
                chunks[rid] = joined
                buf[rid] = []
        pos = e
    if pos < len(translated):
        if stack:
            buf.setdefault(stack[-1], []).append(translated[pos:])
        else:
            outside.append(translated[pos:])

    # ë‹«íˆì§€ ì•Šì€ ë²„í¼ ì²˜ë¦¬
    for rid, lst in buf.items():
        if lst:
            chunks[rid] = chunks.get(rid, "") + "".join(lst)

    has_outside = any(t.strip() for t in outside)
    return ids, chunks, has_outside


def try_inplace_update_paragraph(paragraph, translated, font_scale=1.0):
    """ë§ˆì»¤ê°€ 1..Nìœ¼ë¡œ ì •í™•íˆ ì¡´ì¬í•˜ê³ , ë§ˆì»¤ ë°– í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´
    ê¸°ì¡´ runsì— í…ìŠ¤íŠ¸ë§Œ ì£¼ì…í•˜ì—¬ ì„œì‹ì„ 100% ìœ ì§€í•œë‹¤."""
    ids, chunks, has_outside = _parse_run_chunks(translated)
    runs = [r for r in paragraph.runs if (r.text or "") != ""]
    N = len(runs)

    # ì¡°ê±´: ë§ˆì»¤ ë°– í…ìŠ¤íŠ¸ê°€ ì—†ì–´ì•¼ í•˜ê³ , R1..RNì´ ì •í™•íˆ í•œ ë²ˆì”© ì¡´ì¬
    if has_outside or N == 0 or set(ids) != set(range(1, N+1)) or any(ids.count(i) != 1 for i in range(1, N+1)):
        return False

    for i, run in enumerate(runs, start=1):
        run.text = chunks.get(i, "")
        # í°íŠ¸ í¬ê¸° ì¡°ì • ì ìš©
        if font_scale != 1.0 and run.font.size is not None:
            from pptx.util import Pt
            original_size = run.font.size.pt
            adjusted_size = original_size * font_scale
            run.font.size = Pt(adjusted_size)
    return True


# ---------- [í”„ë¡¬í”„íŠ¸ ë¹Œë”] ----------
def build_tone_instructions(tone: str) -> str:
    """
    ì„ íƒí•œ í†¤ì— ë§ëŠ” ì§€ì‹œë¬¸ì„ ë°˜í™˜
    """
    if tone == "ê¸°ë³¸ê°’":
        return (
            "Use a natural, professional beauty-industry tone localized to the target market. "
            "Keep terminology consistent with beauty marketing and professional skincare. "
            "Be clear and persuasive without hype; avoid overpromising."
        )
    if tone == "Med/Pharma Pro (20y)":
        return (
            "Use a formal, clinically precise B2B tone suitable for medical devices and professional pharmacists. "
            "Prioritize clarity, compliance, and evidence-based wording. Avoid hype. "
            "Prefer terminology used in regulatory, clinical, and professional settings."
        )
    if tone == "Beauty Pro (20y, chic)":
        return (
            "Use a refined, polished professional tone common in premium beauty and aesthetic clinics. "
            "Balance expertise with approachable elegance. Maintain brand voice consistency without overpromising."
        )
    if tone == "GenZ Female (20s)":
        return (
            "Use a modern, friendly, and concise tone tailored for women in their 20s. "
            "Be clear and engaging for social content, but avoid slang overload, emojis, and exaggerated claims."
        )
    if tone == "Medical/Science Expert":
        return (
            "Use a precise, scientific, and clinically accurate tone suitable for medical and scientific documentation. "
            "Maintain strict adherence to medical terminology, biological processes, and scientific accuracy. "
            "Preserve technical terms, Latin names, and scientific nomenclature exactly as they appear. "
            "Ensure translations are suitable for medical professionals, researchers, and regulatory submissions."
        )
    # fallback
    return "Use a neutral professional tone appropriate for the beauty industry."


def build_chinese_prompt(tagged_text: str, target_lang: str) -> str:
    """
    ì „ë¬¸ì ì¸ í•œêµ­ì–´â†’ì¤‘êµ­ì–´ ë²ˆì—­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (ê°„ì²´/ë²ˆì²´ êµ¬ë¶„)
    """
    chinese_type = "ê°„ì²´" if "Simplified" in target_lang else "ë²ˆì²´"
    
    return (
        f"ë‹¹ì‹ ì€ í•œêµ­ì–´ë¥¼ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì¤‘êµ­ì–´({chinese_type})ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n\n"
        f"# ì£¼ìš” íŠ¹ì§•\n"
        f"- í”„ë ˆì  í…Œì´ì…˜, ë³´ê³ ì„œ, ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ ë“±ì— ì í•©í•œ ê³µì‹ì ì´ê³  ì„¸ë ¨ëœ í‘œí˜„ ì‚¬ìš©\n"
        f"- ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ë¬¸ì¥ì˜ ì˜ë¯¸ì™€ ë‰˜ì•™ìŠ¤ë¥¼ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í‘œí˜„ìœ¼ë¡œ ë²ˆì—­\n"
        f"- ì›ë¬¸ì˜ ì˜ë„ì™€ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë˜, ì¤‘êµ­ ì›ì–´ë¯¼ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë“¤ë¦¬ë„ë¡ ë¬¸ì¥ êµ¬ì¡° ì¡°ì •\n"
        f"- ë²ˆì—­ ì´ì™¸ì˜ ë¶ˆí•„ìš”í•œ ì„¤ëª… ê¸ˆì§€\n"
        f"- ì°½ì˜ì  ì¬í•´ì„ ì—†ì´ ì›ë¬¸ì— ì¶©ì‹¤í•œ ë²ˆì—­ ìˆ˜í–‰\n"
        f"- ë°˜ë“œì‹œ ì¤‘êµ­ì–´ {chinese_type}ë¡œ ë²ˆì—­í•˜ì„¸ìš”\n\n"
        f"# ê³ ìœ ëª…ì‚¬ ì²˜ë¦¬ ê·œì¹™\n"
        f"- 'í”¼ë”ë¦°'ì€ 'PYDERIN'ìœ¼ë¡œ ë²ˆì—­í•˜ì„¸ìš” (ë¸Œëœë“œëª…ì´ë¯€ë¡œ ëŒ€ë¬¸ìë¡œ)\n"
        f"- ê¸°íƒ€ ê³ ìœ ëª…ì‚¬(ì¸ëª…, ì§€ëª…, íšŒì‚¬ëª…, ë¸Œëœë“œëª… ë“±)ëŠ” ë²ˆì—­í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”\n"
        f"- ì˜ì–´ ê³ ìœ ëª…ì‚¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”\n\n"
        f"ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì¤‘êµ­ì–´({chinese_type})ë¡œ ë²ˆì—­í•˜ì„¸ìš”.\n"
        f"ì¤‘ìš”: [[R1]]...[[/R1]] ê°™ì€ ë§ˆì»¤ íƒœê·¸ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ê±°ë‚˜ ì œê±°í•˜ì§€ ë§ˆì„¸ìš”. ì •í™•íˆ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"ë²ˆì—­í•  í…ìŠ¤íŠ¸:\n{tagged_text}"
    )


def build_medical_science_prompt(tagged_text: str, target_lang: str) -> str:
    """
    ì˜í•™/ê³¼í•™ ì „ë¬¸ ë²ˆì—­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    """
    return (
        f"You are a medical and scientific translation expert with 20+ years of experience. "
        f"Translate the following Korean medical/scientific text into precise, professional {target_lang}. "
        f"Only return the translated text. If there is nothing to translate, return an empty string.\n\n"
        f"# Translation Guidelines:\n"
        f"- Maintain strict scientific accuracy and medical terminology\n"
        f"- Preserve all technical terms, Latin names, and scientific nomenclature exactly\n"
        f"- Use appropriate medical/scientific vocabulary for the target language\n"
        f"- Ensure the translation is suitable for medical professionals and researchers\n"
        f"- Maintain the formal, clinical tone of scientific documentation\n"
        f"- Keep biological processes, anatomical terms, and chemical names precise\n"
        f"- If the source is already in {target_lang}, lightly copyedit for scientific accuracy\n\n"
        f"# Critical Requirements:\n"
        f"- Do NOT alter or remove any marker tags like [[R1]]...[[/R1]]\n"
        f"- Keep all markers exactly as-is and in correct pairs\n"
        f"- Preserve the exact structure and formatting\n\n"
        f"Translate the following text:\n{tagged_text}"
    )


def build_prompt(tagged_text: str, target_lang: str, tone: str) -> str:
    # Medical/Science Expert uses specialized prompt
    if tone == "Medical/Science Expert":
        return build_medical_science_prompt(tagged_text, target_lang)
    
    # Chinese translation uses specialized prompt
    if "Chinese" in target_lang:
        return build_chinese_prompt(tagged_text, target_lang)
    
    tone_text = build_tone_instructions(tone)
    return (
        f"Translate the following beauty industry presentation text into natural, professional {target_lang}. "
        f"Only return the translated text. If there is nothing to translate, return an empty string. "
        f"Context: {tone_text} "
        f"Avoid literal translationâ€”use expressions that sound natural for beauty marketing and professional skincare. "
        f"If the source is already in {target_lang}, lightly copyedit for clarity, consistency, and terminology. "
        f"CRITICAL: Do NOT alter or remove any marker tags like [[R1]]...[[/R1]]. Keep them exactly as-is and in correct pairs. "
        f"Return the translated text with all markers preserved:\n\n{tagged_text}"
    )


# ---------- [ë²ˆì—­ í˜¸ì¶œ] ----------
def gpt_translate_tagged(tagged_text: str, client, target_lang: str, tone: str, use_deepseek=False) -> str:
    # ì§„ì§œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë²ˆì—­ ìŠ¤í‚µ
    if not tagged_text.strip() or is_effectively_empty_tagged(tagged_text):
        return ""

    # ì¤‘êµ­ì–´ ë²ˆì—­ì˜ ê²½ìš° DeepSeek ì‚¬ìš©
    if "Chinese" in target_lang and use_deepseek:
        deepseek_client = create_deepseek_client(client.api_key)
        prompt = build_chinese_prompt(tagged_text, target_lang)
        content = safe_request(deepseek_client, prompt, retries=3, delay=3, use_deepseek=True)
    else:
        prompt = build_prompt(tagged_text, target_lang, tone)
        content = safe_request(client, prompt, retries=3, delay=3)

    # ì‹¤íŒ¨ ì‹œ ì›ë¬¸(ë§ˆì»¤ í¬í•¨) ë°˜í™˜ â†’ ì›ë¬¸ ìœ ì§€
    if not content:
        return tagged_text

    # ì‚¬ê³¼ë¬¸/ì—ëŸ¬ë¬¸êµ¬ê°€ ë“¤ì–´ì˜¤ë©´ ì›ë¬¸ ìœ ì§€
    if looks_like_apology(content):
        return tagged_text

    return content


def process_nested_shapes(shapes, target_lang, tone, client, use_deepseek, font_scale, progress_callback, slide_num, total_slides, shape_path="", should_stop=None):
    """ì¤‘ì²©ëœ shapeë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬"""
    for shape_idx, shape in enumerate(shapes):
        # ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
        if should_stop and should_stop():
            print("   â¹ï¸ ë²ˆì—­ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
            
        current_path = f"{shape_path}.{shape_idx}" if shape_path else str(shape_idx)
        
        # í…ìŠ¤íŠ¸ í”„ë ˆì„ ì²˜ë¦¬
        if getattr(shape, "has_text_frame", False) and shape.has_text_frame:
            tf = shape.text_frame
            for p_idx, p in enumerate(tf.paragraphs):
                tagged, style_map = tag_paragraph(p)
                if not tagged:
                    continue
                preview = (tagged[:40] + "...") if len(tagged) > 40 else tagged
                print(f"   ğŸ”¤ ë²ˆì—­ ì¤‘(ì¤‘ì²©í…ìŠ¤íŠ¸): {preview}")
                if progress_callback:
                    progress_callback(slide_num, total_slides, f"ì¤‘ì²© í…ìŠ¤íŠ¸ ë²ˆì—­ ì¤‘: {preview}")
                
                translated = gpt_translate_tagged(tagged, client, target_lang, tone, use_deepseek)
                translated = translated.strip().strip('"').strip("'")
                
                if not try_inplace_update_paragraph(p, translated, font_scale):
                    rebuild_paragraph_from_tagged(p, translated, style_map, font_scale)
                
                time.sleep(SLEEP_SEC)
        
        # í‘œ ì²˜ë¦¬ (ì¤‘ì²©ëœ í‘œ í¬í•¨)
        elif getattr(shape, "has_table", False) and shape.has_table:
            print(f"   ğŸ“Š í‘œ ì²˜ë¦¬ ì¤‘ (ê²½ë¡œ: {current_path})")
            for row_idx, row in enumerate(shape.table.rows):
                for cell_idx, cell in enumerate(row.cells):
                    # ì…€ì˜ í…ìŠ¤íŠ¸ í”„ë ˆì„ ì²˜ë¦¬
                    if getattr(cell, "text_frame", None):
                        tf = cell.text_frame
                        for p_idx, p in enumerate(tf.paragraphs):
                            tagged, style_map = tag_paragraph(p)
                            if not tagged:
                                continue
                            translated = gpt_translate_tagged(tagged, client, target_lang, tone, use_deepseek)
                            translated = translated.strip().strip('"').strip("'")
                            
                            if not try_inplace_update_paragraph(p, translated, font_scale):
                                rebuild_paragraph_from_tagged(p, translated, style_map, font_scale)
                            time.sleep(SLEEP_SEC)
                    
                    # ì…€ ì•ˆì˜ ë‹¤ë¥¸ shapeë“¤ ì²˜ë¦¬ (ì¤‘ì²©ëœ í‘œ, í…ìŠ¤íŠ¸ë°•ìŠ¤ ë“±)
                    if hasattr(cell, 'shapes') and cell.shapes:
                        print(f"     ğŸ” ì…€ ë‚´ë¶€ shape ë°œê²¬ (í–‰:{row_idx}, ì—´:{cell_idx})")
                        if not process_nested_shapes(
                            cell.shapes, target_lang, tone, client, use_deepseek, 
                            font_scale, progress_callback, slide_num, total_slides, f"{current_path}.table_{row_idx}_{cell_idx}", should_stop
                        ):
                            return False
        
        # ê¸°íƒ€ shape íƒ€ì…ë“¤ë„ í™•ì¸ (ê·¸ë£¹, í…ìŠ¤íŠ¸ë°•ìŠ¤ ë“±)
        elif hasattr(shape, 'shapes') and shape.shapes:
            print(f"   ğŸ” ê·¸ë£¹ shape ë°œê²¬ (ê²½ë¡œ: {current_path})")
            if not process_nested_shapes(
                shape.shapes, target_lang, tone, client, use_deepseek, 
                font_scale, progress_callback, slide_num, total_slides, current_path, should_stop
            ):
                return False
    
    return True


def translate_presentation(pptx_path: str, target_lang: str, tone: str, openai_api_key: str, deepseek_api_key: str, use_deepseek=False, progress_callback=None, font_scale=1.0, use_smart_grouping=True, should_stop=None):
    """
    í”„ë ˆì  í…Œì´ì…˜ì„ ë²ˆì—­í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    progress_callback: (current_slide, total_slides, current_text) -> None
    """
    print(f"ğŸ“‚ íŒŒì¼: {pptx_path}")
    print(f"ğŸŒ ëŒ€ìƒ ì–¸ì–´: {target_lang}")
    print(f"ğŸ™ í†¤: {tone}")
    if target_lang == "Chinese" and use_deepseek:
        print("ğŸ¤– DeepSeek ëª¨ë¸ ì‚¬ìš© ì¤‘...")
    else:
        print("ğŸ”‘ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")

    client = create_openai_client(openai_api_key)

    print("ğŸ“– í”„ë ˆì  í…Œì´ì…˜ ë¡œë”©...")
    pres = Presentation(pptx_path)

    slide_count = len(pres.slides)
    print(f"ğŸ–¼ ìŠ¬ë¼ì´ë“œ ìˆ˜: {slide_count}")
    
    # ì›ë³¸ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°±ì—… (ì¤‘êµ­ì–´ ë²ˆì—­ìš©)
    original_korean_backup = {}
    if "Chinese" in target_lang:
        chinese_type = "ê°„ì²´" if "Simplified" in target_lang else "ë²ˆì²´"
        print(f"ğŸ“ ì›ë³¸ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°±ì—… ì¤‘... (ì¤‘êµ­ì–´ {chinese_type} ë²ˆì—­ìš©)")
        for s_idx, slide in enumerate(pres.slides, start=1):
            original_korean_backup[s_idx] = {}
            for shape_idx, shape in enumerate(slide.shapes):
                if getattr(shape, "has_text_frame", False) and shape.has_text_frame:
                    tf = shape.text_frame
                    for p_idx, p in enumerate(tf.paragraphs):
                        tagged, _ = tag_paragraph(p)
                        if tagged:
                            original_korean_backup[s_idx][f"{shape_idx}_{p_idx}"] = tagged
                elif getattr(shape, "has_table", False) and shape.has_table:
                    for row_idx, row in enumerate(shape.table.rows):
                        for cell_idx, cell in enumerate(row.cells):
                            if not getattr(cell, "text_frame", None):
                                continue
                            tf = cell.text_frame
                            for p_idx, p in enumerate(tf.paragraphs):
                                tagged, _ = tag_paragraph(p)
                                if tagged:
                                    original_korean_backup[s_idx][f"table_{row_idx}_{cell_idx}_{p_idx}"] = tagged
    
    print("-" * 60)

    for s_idx, slide in enumerate(pres.slides, start=1):
        print(f"â–¶ ìŠ¬ë¼ì´ë“œ {s_idx}/{slide_count}")
        if progress_callback:
            progress_callback(s_idx, slide_count, f"ìŠ¬ë¼ì´ë“œ {s_idx} ì²˜ë¦¬ ì¤‘...")

        # ì¤‘ì§€ ì‹ í˜¸ í™•ì¸
        if should_stop and should_stop():
            print(f"â¹ï¸ ìŠ¬ë¼ì´ë“œ {s_idx}ì—ì„œ ë²ˆì—­ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return None
        
        # ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        if use_smart_grouping:
            # ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ë¨¼ì € ì‹œë„
            smart_grouping_success = apply_smart_grouping_to_slide(
                slide, target_lang, tone, client, use_deepseek, font_scale
            )
            
            if not smart_grouping_success:
                # ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                print(f"   ğŸ”„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ìŠ¬ë¼ì´ë“œ {s_idx} ì²˜ë¦¬ ì¤‘...")
                if not process_nested_shapes(
                    slide.shapes, target_lang, tone, client, use_deepseek, 
                    font_scale, progress_callback, s_idx, slide_count, "", should_stop
                ):
                    print(f"â¹ï¸ ìŠ¬ë¼ì´ë“œ {s_idx}ì—ì„œ ë²ˆì—­ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return None
        else:
            # ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ë¹„í™œì„±í™” ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            print(f"   ğŸ”„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ìŠ¬ë¼ì´ë“œ {s_idx} ì²˜ë¦¬ ì¤‘...")
            if not process_nested_shapes(
                slide.shapes, target_lang, tone, client, use_deepseek, 
                font_scale, progress_callback, s_idx, slide_count, "", should_stop
            ):
                print(f"â¹ï¸ ìŠ¬ë¼ì´ë“œ {s_idx}ì—ì„œ ë²ˆì—­ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return None

    folder = os.path.dirname(pptx_path)
    stem, ext = os.path.splitext(os.path.basename(pptx_path))
    
    # ì¤‘êµ­ì–´ ë²ˆì—­ì˜ ê²½ìš° í†¤ ëŒ€ì‹  ì¤‘êµ­ì–´ íƒ€ì…ì„ ì‚¬ìš©
    if "Chinese" in target_lang:
        chinese_type = "Simplified" if "Simplified" in target_lang else "Traditional"
        outfile_name = f"{stem}_Chinese_{chinese_type}_AIë²ˆì—­ì™„ë£Œ{ext}"
    else:
        safe_tone = re.sub(r'[^0-9A-Za-zê°€-í£_()+-]', '', tone.replace(' ', ''))
        outfile_name = f"{stem}_{target_lang}_{safe_tone}_AIë²ˆì—­ì™„ë£Œ{ext}"
    
    outfile_path = os.path.join(folder, outfile_name)
    outfile_path = unique_path(outfile_path)

    print("-" * 60)
    print("ğŸ’¾ ì €ì¥ ì¤‘...")
    pres.save(outfile_path)
    print(f"âœ… ë²ˆì—­ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {outfile_path}")
    
    return outfile_path


# ========== [ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ê¸°ëŠ¥] ==========

def get_text_box_metadata(text_box):
    """í…ìŠ¤íŠ¸ë°•ìŠ¤ì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    metadata = {
        "text": "",
        "position": {"left": 0, "top": 0, "width": 0, "height": 0},
        "style": {},
        "has_text_frame": False
    }
    
    # ìœ„ì¹˜ ì •ë³´
    if hasattr(text_box, 'left'):
        metadata["position"]["left"] = text_box.left
    if hasattr(text_box, 'top'):
        metadata["position"]["top"] = text_box.top
    if hasattr(text_box, 'width'):
        metadata["position"]["width"] = text_box.width
    if hasattr(text_box, 'height'):
        metadata["position"]["height"] = text_box.height
    
    # í…ìŠ¤íŠ¸ ë° ìŠ¤íƒ€ì¼ ì •ë³´
    if hasattr(text_box, 'text_frame') and text_box.text_frame:
        metadata["has_text_frame"] = True
        metadata["text"] = text_box.text_frame.text
        
        # ì²« ë²ˆì§¸ ë¬¸ë‹¨ì˜ ì²« ë²ˆì§¸ runì˜ ìŠ¤íƒ€ì¼
        if text_box.text_frame.paragraphs and text_box.text_frame.paragraphs[0].runs:
            first_run = text_box.text_frame.paragraphs[0].runs[0]
            # ì•ˆì „í•œ ìƒ‰ìƒ ì •ë³´ ì¶”ì¶œ
            font_color = None
            if first_run.font.color:
                try:
                    if hasattr(first_run.font.color, 'rgb') and first_run.font.color.rgb:
                        font_color = str(first_run.font.color.rgb)
                    elif hasattr(first_run.font.color, 'theme_color'):
                        font_color = f"theme_color_{first_run.font.color.theme_color}"
                except (AttributeError, TypeError):
                    font_color = None
            
            metadata["style"] = {
                "font_name": first_run.font.name,
                "font_size": first_run.font.size.pt if first_run.font.size else None,
                "font_color": font_color,
                "bold": first_run.font.bold,
                "italic": first_run.font.italic,
                "underline": first_run.font.underline
            }
    
    return metadata


def ai_analyze_text_grouping(text_metadata_list, slide_context=""):
    """AIê°€ í…ìŠ¤íŠ¸ë°•ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ ê·¸ë£¹í•‘ ê²°ì • (GPT-5 ì‚¬ìš©)"""
    
    # í…ìŠ¤íŠ¸ ëª©ë¡ ìƒì„±
    texts = [meta["text"] for meta in text_metadata_list if meta["text"].strip()]
    
    if not texts:
        return []
    
    prompt = f"""
ë‹¤ìŒ ìŠ¬ë¼ì´ë“œì˜ í…ìŠ¤íŠ¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ê·¸ë£¹í•‘í•´ì£¼ì„¸ìš”:

í…ìŠ¤íŠ¸ë“¤: {texts}
ìŠ¬ë¼ì´ë“œ ì»¨í…ìŠ¤íŠ¸: {slide_context}

ê·¸ë£¹í•‘ ê·œì¹™:
1. ë¬¸ë§¥ìƒ ì—°ê²°ëœ í…ìŠ¤íŠ¸ë“¤ì€ ê°™ì€ ê·¸ë£¹ (ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”" + "ë°˜ê°‘ìŠµë‹ˆë‹¤")
2. ë…ë¦½ì ì¸ ì •ë³´ ë‹¨ìœ„ëŠ” ê°œë³„ ê·¸ë£¹ (ì˜ˆ: "ì œí’ˆëª…", "ê°€ê²©")
3. ìŠ¤íƒ€ì¼ë§ì„ ìœ„í•œ ì˜ë„ì  ë¶„í• ì€ ê°œë³„ ê·¸ë£¹ (ì˜ˆ: "100" + "ê°œ", "$" + "50")
4. ìˆ«ì+ë‹¨ìœ„, í†µí™”+ê¸ˆì•¡, ë¼ë²¨+ê°’ ë“±ì€ ë¶„ë¦¬
5. ì™„ì „í•œ ë¬¸ì¥ì˜ ì¼ë¶€ì¸ ê²½ìš°ë§Œ ê·¸ë£¹í•‘

ê·¸ë£¹í•‘ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì¶œë ¥:
[
    {{"group": 1, "text_indices": [0, 1, 2]}},
    {{"group": 2, "text_indices": [3]}},
    {{"group": 3, "text_indices": [4, 5]}}
]

í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
"""

    try:
        from openai import OpenAI
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1000,
            temperature=0.1
        )
        
        content = response.choices[0].message.content.strip()
        
        # JSON íŒŒì‹±
        import json
        import re
        
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            groups = json.loads(json_str)
            return groups
        else:
            print(f"âš ï¸ AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {content}")
            return []
            
    except Exception as e:
        print(f"âŒ AI ê·¸ë£¹í•‘ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def smart_split_translation(translated_text, num_parts):
    """ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í•  (GPT-5 ì‚¬ìš©)"""
    
    if num_parts <= 1:
        return [translated_text]
    
    prompt = f"""
ë‹¤ìŒ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ {num_parts}ê°œì˜ ë¶€ë¶„ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• í•´ì£¼ì„¸ìš”:

ë²ˆì—­ëœ í…ìŠ¤íŠ¸: "{translated_text}"
ë¶„í•  ê°œìˆ˜: {num_parts}

ê° ë¶€ë¶„ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì´ë‚˜ êµ¬ë¬¸ì´ ë˜ë„ë¡ ë¶„í• í•´ì£¼ì„¸ìš”.
ì˜ë¯¸ ë‹¨ìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ ë¶„í• í•˜ì„¸ìš”.

ë¶„í•  ê²°ê³¼ë¥¼ JSON ë°°ì—´ë¡œ ì¶œë ¥:
["ì²« ë²ˆì§¸ ë¶€ë¶„", "ë‘ ë²ˆì§¸ ë¶€ë¶„", "ì„¸ ë²ˆì§¸ ë¶€ë¶„"]
"""

    try:
        from openai import OpenAI
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.1
        )
        
        content = response.choices[0].message.content.strip()
        
        # JSON íŒŒì‹±
        import json
        import re
        
        # JSON ë°°ì—´ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            split_texts = json.loads(json_str)
            
            # ê°œìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ë¶„í• 
            if len(split_texts) != num_parts:
                print(f"âš ï¸ AI ë¶„í•  ê²°ê³¼ ê°œìˆ˜ ë¶ˆì¼ì¹˜: {len(split_texts)} != {num_parts}")
                return manual_split_text(translated_text, num_parts)
            
            return split_texts
        else:
            print(f"âš ï¸ AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {content}")
            return manual_split_text(translated_text, num_parts)
            
    except Exception as e:
        print(f"âŒ AI í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
        return manual_split_text(translated_text, num_parts)


def manual_split_text(text, num_parts):
    """ìˆ˜ë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í•  (AI ì‹¤íŒ¨ ì‹œ ë°±ì—…)"""
    if num_parts <= 1:
        return [text]
    
    # ê³µë°±ìœ¼ë¡œ ë¶„í• 
    words = text.split()
    if len(words) <= num_parts:
        return words + [""] * (num_parts - len(words))
    
    # ê· ë“±í•˜ê²Œ ë¶„í• 
    words_per_part = len(words) // num_parts
    result = []
    
    for i in range(num_parts):
        start = i * words_per_part
        if i == num_parts - 1:  # ë§ˆì§€ë§‰ ë¶€ë¶„ì€ ë‚˜ë¨¸ì§€ ëª¨ë“  ë‹¨ì–´
            end = len(words)
        else:
            end = (i + 1) * words_per_part
        
        part = " ".join(words[start:end])
        result.append(part)
    
    return result


def translate_text_group_with_style_preservation(group_metadata, target_lang, tone, client, use_deepseek=False):
    """í…ìŠ¤íŠ¸ ê·¸ë£¹ì„ ë²ˆì—­í•˜ë˜ ê° í…ìŠ¤íŠ¸ë°•ìŠ¤ì˜ ìŠ¤íƒ€ì¼ ë³´ì¡´"""
    
    if not group_metadata:
        return []
    
    # 1. ê·¸ë£¹ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨
    combined_text = " ".join([meta["text"] for meta in group_metadata if meta["text"].strip()])
    
    if not combined_text.strip():
        return group_metadata
    
    # 2. ì „ì²´ ë¬¸ë§¥ìœ¼ë¡œ ë²ˆì—­
    print(f"   ğŸ”¤ ê·¸ë£¹ ë²ˆì—­ ì¤‘: {combined_text[:50]}...")
    
    # íƒœê·¸ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸°ì¡´ ë¡œì§ í™œìš©)
    tagged_text = f"[[R1]]{combined_text}[[/R1]]"
    translated = gpt_translate_tagged(tagged_text, client, target_lang, tone, use_deepseek)
    translated = translated.strip().strip('"').strip("'")
    
    # 3. ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì›ë˜ í…ìŠ¤íŠ¸ë°•ìŠ¤ ê°œìˆ˜ë§Œí¼ ë¶„í• 
    num_parts = len(group_metadata)
    split_texts = smart_split_translation(translated, num_parts)
    
    # 4. ê° í…ìŠ¤íŠ¸ë°•ìŠ¤ì— ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì™€ ì›ë˜ ìŠ¤íƒ€ì¼ ì ìš©
    result = []
    for i, box_meta in enumerate(group_metadata):
        translated_text = split_texts[i] if i < len(split_texts) else ""
        
        result.append({
            "text": translated_text,
            "position": box_meta["position"],
            "style": box_meta["style"],
            "has_text_frame": box_meta["has_text_frame"]
        })
    
    return result


def apply_smart_grouping_to_slide(slide, target_lang, tone, client, use_deepseek=False, font_scale=1.0):
    """ìŠ¬ë¼ì´ë“œì— ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ì ìš©"""
    
    print(f"   ğŸ§  ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ì ìš© ì¤‘...")
    
    # 1. ëª¨ë“  í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜ì§‘ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    text_boxes = []
    text_metadata = []
    
    for shape in slide.shapes:
        if hasattr(shape, 'text_frame') and shape.text_frame and shape.text_frame.text.strip():
            text_boxes.append(shape)
            metadata = get_text_box_metadata(shape)
            text_metadata.append(metadata)
    
    if not text_metadata:
        return True
    
    # 2. AI ê¸°ë°˜ ê·¸ë£¹í•‘ ë¶„ì„
    groups = ai_analyze_text_grouping(text_metadata, f"ìŠ¬ë¼ì´ë“œ {slide.slide_id}")
    
    if not groups:
        print(f"   âš ï¸ AI ê·¸ë£¹í•‘ ì‹¤íŒ¨, ê°œë³„ ë²ˆì—­ìœ¼ë¡œ ì§„í–‰")
        return False
    
    # 3. ê° ê·¸ë£¹ ë²ˆì—­ ë° ì ìš©
    for group_info in groups:
        group_indices = group_info.get("text_indices", [])
        if not group_indices:
            continue
        
        # ê·¸ë£¹ì˜ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        group_metadata = [text_metadata[i] for i in group_indices if i < len(text_metadata)]
        
        if len(group_metadata) <= 1:
            # ë‹¨ì¼ í…ìŠ¤íŠ¸ë°•ìŠ¤ëŠ” ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            continue
        
        # ê·¸ë£¹ ë²ˆì—­
        translated_group = translate_text_group_with_style_preservation(
            group_metadata, target_lang, tone, client, use_deepseek
        )
        
        # ì›ë˜ í…ìŠ¤íŠ¸ë°•ìŠ¤ì— ì ìš©
        for i, box_idx in enumerate(group_indices):
            if i < len(translated_group) and box_idx < len(text_boxes):
                original_box = text_boxes[box_idx]
                translated_text = translated_group[i]["text"]
                
                # í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                update_text_box_with_translation(original_box, translated_text, font_scale)
    
    print(f"   âœ… ìŠ¤ë§ˆíŠ¸ ê·¸ë£¹í•‘ ì™„ë£Œ")
    return True


def update_text_box_with_translation(text_box, translated_text, font_scale=1.0):
    """í…ìŠ¤íŠ¸ë°•ìŠ¤ì— ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì ìš©"""
    
    if not hasattr(text_box, 'text_frame') or not text_box.text_frame:
        return
    
    # ê¸°ì¡´ ë‚´ìš©ì„ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¡œ êµì²´
    for paragraph in text_box.text_frame.paragraphs:
        for run in paragraph.runs:
            run.text = ""
    
    # ì²« ë²ˆì§¸ ë¬¸ë‹¨ì˜ ì²« ë²ˆì§¸ runì— ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì ìš©
    if text_box.text_frame.paragraphs and text_box.text_frame.paragraphs[0].runs:
        first_run = text_box.text_frame.paragraphs[0].runs[0]
        first_run.text = translated_text
        
        # í°íŠ¸ í¬ê¸° ì¡°ì •
        if font_scale != 1.0 and first_run.font.size:
            from pptx.util import Pt
            original_size = first_run.font.size.pt
            adjusted_size = original_size * font_scale
            first_run.font.size = Pt(adjusted_size)
